# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBe6PlRY0GxT00Okl1N8J0I_a4TfIo6S
"""

!pip install requests
!pip install bs4
!pip install chardet
!pip install nltk

from bs4 import BeautifulSoup
import requests
import pandas as pd
import numpy as np
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

input = pd.read_excel("Input_file.xlsx")
a = []
article_head = []
for i in range(len(input['URL'])):
  try:
    source = requests.get(input['URL'][i])
    source.raise_for_status()
    soup = BeautifulSoup(source.text, 'html.parser')
    classes_list = ["td-post-content tagdiv-type","tdb-block-inner td-fix-index"]
    for classes in classes_list:
      article_text = soup.find('div', class_ = classes)
      if article_text:
        article_text = article_text.text.strip()
        break
    article_head.append(soup.find('title'))
    a.append(article_text.replace('\n',' '))
    file_name = '/content/sample_data/Extracted_article' + str(input['URL_ID'][i]) + '.txt'
    with open(file_name, 'w') as file:
      file.write(str(article_head[i]) + '\n' + str(article_text))
  except Exception as e:
    a.append("404 Client Error")
    article_head.append("404 Client Error")
    file_name = '/content/sample_data/Extracted_article' + str(input['URL_ID'][i]) + '.txt'
    with open(file_name, 'w') as file:
      file.write(str(article_head[i]) + '\n' + str(article_text))

head = pd.DataFrame(article_head)
text = pd.DataFrame(a)
lst_text = list(a)
final = pd.concat([head,text], axis=1)

final

import chardet

def load_stop_words(stop_words_file):
    with open(stop_words_file, 'rb') as file:
        result = chardet.detect(file.read())
    file_encoding = result['encoding']

    with open(stop_words_file, 'r', encoding=file_encoding) as file:
        lines = file.readlines()
    first_parts = [line.split('|')[0].strip().lower() for line in lines]
    return first_parts

a1 = load_stop_words("StopWords_Auditor.txt")
b = load_stop_words("StopWords_DatesandNumbers.txt")
c = load_stop_words("StopWords_Generic.txt")
d = load_stop_words("StopWords_GenericLong.txt")
e = load_stop_words("StopWords_Geographic.txt")
f = load_stop_words("StopWords_Names.txt")
g = load_stop_words("StopWords_Currencies.txt")
stopwords_all = a1+b+c+d+e+f+g
stopwords_all = list(set(stopwords_all))

positive_words = list(set(load_stop_words("positive-words.txt")))
negative_words = list(set(load_stop_words("negative-words.txt")))

b = []
for i in range(len(text)):
  tokens = word_tokenize(lst_text[i])
  tokenized_words = [word for word in tokens if word.lower() not in stopwords_all]
  b.append(tokenized_words)

positive_words_of_token = []
negative_words_of_token =[]
positive_score = []
negative_score = []
polarity_score = []
subjectivity_score = []
avg_sentence_length = []
percentage_of_complex_words = []
fog_index = []
avg_no_of_words_in_sentence = []
complex_word_count = []
word_count = []
syllable_per_word = []
personal_pronouns = []
avg_word_length = []
stopwords_from_nltk = set(stopwords.words("english"))
b_nltk = []
for i in range(len(text)):
  filtered_word = re.sub(r'[^\w\s]', '',lst_text[i])
  tokens = word_tokenize(filtered_word)
  tokenized_words = [word for word in tokens if word.lower() not in stopwords_from_nltk]
  b_nltk.append(tokenized_words)

for i in range(len(b)):
  positive_words_of_token.append([word for word in b[i] if word.lower() in positive_words])
  negative_words_of_token.append([word for word in b[i] if word.lower() in negative_words])
  positive_score.append(len(positive_words_of_token[i]))
  negative_score.append(len(negative_words_of_token[i]))
  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))
  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(b[i])) + 0.000001))

for i in range(len(lst_text)):
  avg_sentence_length.append(len(lst_text[i])/len(lst_text[i].split(".")))
  count = sum(1 for letter in lst_text[i].split(" ") if letter.lower() in 'aeiou')
  if count>2:
    complex_word_count.append(count)
  else:
    complex_word_count.append(0)
  percentage_of_complex_words.append(complex_word_count[i]/len(lst_text[i].split(" ")))
  fog_index.append(0.4*(avg_sentence_length[i]+percentage_of_complex_words[i]))
  avg_no_of_words_in_sentence.append(len(lst_text[i].split(" "))/len(lst_text[i].split(".")))
  word_count.append(len(b_nltk[i]))
  words_for_syllable = [word for word in lst_text[i].split(" ")]
  syllable_count = 0
  syllable_for_each_word = []
  for word in words_for_syllable:
    word1 = word
    if word.endswith('ed') or word.endswith('es'):
      word1 = word[:-2]
    syllable_count = sum( 1 for letter in word1 if letter.lower() in 'aeiou')
    syllable_for_each_word.append(syllable_count)
  syllable_per_word.append(syllable_for_each_word)
  pronouns = ["I","we","my","ours","us"]
  county = 0
  for pr in pronouns:
    county += len(re.findall(r"\b" + pr + r"\b", lst_text[i]))
  personal_pronouns.append(county)
  avg_word_length.append(len(lst_text[i])/len(lst_text[i].split(' ')))

output_df = pd.read_excel('Output Data Structure.xlsx')
variables = [positive_score,
            negative_score,
            polarity_score,
            subjectivity_score,
            avg_sentence_length,
            percentage_of_complex_words,
            fog_index,
            avg_sentence_length,
            complex_word_count,
            word_count,
            syllable_per_word,
            personal_pronouns,
            avg_word_length]
for i, var in enumerate(variables):
  output_df.iloc[:,i+2] = var
output_df.to_csv('Output_Data.csv')